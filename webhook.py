from flask import Flask, request, jsonify
from fuzzywuzzy import fuzz
import os
from openai import OpenAI
import json
import re
from datetime import datetime, timedelta
from threading import Thread
import requests

# å„²å­˜ä½¿ç”¨è€…å°è©±æ­·å²ï¼Œæ ¼å¼ç‚º {session_id: {"messages": [...], "last_seen": datetime}}
session_histories = {}
MAX_HISTORY = 5  # æœ€å¤šç´€éŒ„ 5 è¼ªï¼ˆuser + assistantï¼‰
SESSION_TIMEOUT = timedelta(minutes=5)

app = Flask(__name__)

# è¨­ç½® OpenAI API å¯†é‘°
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")

# ä¸Šå‚³ PDF æª”æ¡ˆè‡³ OpenAI
# with open(r"C:\Users\N000135995\Documents\Class Index1.pdf", "rb") as f:
#     upload_response = client.files.create(file=f, purpose="assistants")

# file_id = upload_response.id

if client.api_key:
    print("âœ… æˆåŠŸæŠ“åˆ° OPENAI_API_KEY:", client.api_key[:5] + "...")
else:
    print("âŒ æ²’æœ‰æ‰¾åˆ° OPENAI_API_KEY")
    
if LINE_CHANNEL_ACCESS_TOKEN:
    print("âœ… æˆåŠŸæŠ“åˆ° LINE_CHANNEL_ACCESS_TOKEN:", LINE_CHANNEL_ACCESS_TOKEN[:5] + "...")
else:
    print("âŒ æ²’æœ‰æ‰¾åˆ° LINE_CHANNEL_ACCESS_TOKEN")

# è¼‰å…¥ TYPE å’Œé€£çµçš„å°æ‡‰é—œä¿‚
try:
    with open("links.json", "r", encoding="utf-8") as f:
        type_links = json.load(f)
except UnicodeDecodeError as e:
    print(f"è®€å– links.json æ™‚ç™¼ç”Ÿç·¨ç¢¼éŒ¯èª¤ï¼š{e}")
    type_links = {}

# è¼‰å…¥é…ç®¡è©¦å£“è¦ç¯„ JSON
try:
    with open("piping_specification.json", "r", encoding="utf-8") as f:
        piping_specification = json.load(f)
except FileNotFoundError:
    piping_specification = {}
    print("âŒ ç„¡æ³•æ‰¾åˆ°é…ç®¡è¦ç¯„ JSON æª”æ¡ˆã€‚")

try:
    with open("piping_heat_treatment.json", "r", encoding="utf-8") as f:
        piping_heat_treatment = json.load(f)
except FileNotFoundError:
    piping_heat_treatment = {}
    print("âŒ ç„¡æ³•æ‰¾åˆ°ç†±è™•ç†è¦ç¯„ JSON æª”æ¡ˆã€‚")

#å•é¡Œä¸­æ–‡è½‰è‹±æ–‡
def translate_to_english(query):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "è«‹å°‡ä¸‹é¢çš„ä¸­æ–‡å·¥ç¨‹å•é¡Œç¿»è­¯ç‚ºç°¡æ½”ç²¾ç¢ºçš„è‹±æ–‡ï¼Œä¾›è³‡æ–™æ¯”å°ä½¿ç”¨ã€‚"},
            {"role": "user", "content": query}
        ],
        temperature=0.2
    )
    return response.choices[0].message.content.strip()

def search_piping_spec(question, spec_data, keywords, threshold=70):
    if question.startswith("PCQ-"):
        question = question.replace("PCQ-", "", 1)
    question_cleaned = re.sub(r"\s+", "", question).lower()

    matched_summaries = []
    matched_details = {}
    total_matches = 0

    for chapter, data in spec_data.items():
        title = data.get("title", "")
        content = data.get("content", {})

        for sec_num, sec_text in content.items():
            text_clean = re.sub(r"\s+", "", sec_text).lower()

            # æ¨¡ç³Šæ¯”å°åˆ†æ•¸
            score = fuzz.partial_ratio(question_cleaned, text_clean)

            if score >= threshold:
                key = f"ç¬¬{chapter}ç«  {title} - {sec_num}"
                matched_summaries.append(key)
                matched_details[key] = sec_text
                total_matches += 1

    if matched_summaries:
        summary = "\n".join([f"{i+1}. {s}" for i, s in enumerate(matched_summaries)])
        return summary, matched_details, total_matches

    return "æŸ¥ç„¡ç›¸é—œå…§å®¹ã€‚", {}, 0

#LINE æŒ‰éˆ•ç¨‹å¼
def payload_with_buttons(text, options):    
    return {
        "payload": {
            "line": {
                "type": "template",
                "altText": text,
                "template": {
                    "type": "buttons",
                    "text": text,
                    "actions": [
                        {"type": "message", "label": opt, "text": opt} for opt in options
                    ]
                }
            }
        }
    }

def query_download_link(category, source):
    links = {
        ("ç®¡æ”¯æ’", "å¡‘åŒ–"): "https://tinyurl.com/5vk67ywh",
        ("ç®¡æ”¯æ’", "ä¼æ¥­"): "https://tinyurl.com/msxhmnha"
    }
    return links.get((category, source), "æŸ¥ç„¡å°æ‡‰çš„ä¸‹è¼‰é€£çµ")

    # å®šç¾© categories_mapï¼Œé¡ä¼¼ actions_map çš„çµæ§‹
category_keywords = {        "ç®¡æ”¯æ’": ["ç®¡æ”¯æ’", "æ”¯æ’", "ç®¡é“æ”¯æ’","PIPING SUPPORT","SUPPORT"],    } 
action_keywords = {
    "è©¢å•å…§å®¹": ["æŸ¥è©¢", "æŸ¥", "è©¢å•", "æ‰¾"],
    "ä¸‹è¼‰": ["ä¸‹è¼‰", "çµ¦æˆ‘", "æä¾›"],}
sources = ["ä¼æ¥­", "å¡‘åŒ–"]
categories_map = {k: v for v, keys in category_keywords.items() for k in keys}
actions_map = {k: v for v, keys in action_keywords.items() for k in keys}


def extract_from_query(text):
    found = {"category": "", "source": "", "action": ""}

        # æª¢æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„ category
    for keyword, category in categories_map.items():
        if keyword in text:
            found["category"] = category
            break
    for s in sources:
        if s in text:
            found["source"] = s
            break
    if any(word in text for word in action_keywords["ä¸‹è¼‰"]):
        found["action"] = "ä¸‹è¼‰"
    elif any(word in text for word in action_keywords["è©¢å•å…§å®¹"]):
        found["action"] = "è©¢å•å…§å®¹"

    return found
       # è®€å– context ä¸­çš„åƒæ•¸

@app.route("/webhook", methods=["POST"])
def webhook():
    req = request.get_json()
    if not isinstance(req, dict):
        print(f"âŒ éŒ¯èª¤ï¼šreq ä¸æ˜¯å­—å…¸ï¼Œè€Œæ˜¯ {type(req)}")
        return jsonify({"fulfillmentText": "è«‹æ±‚æ ¼å¼éŒ¯èª¤ï¼Œè«‹ç¢ºä¿ Content-Type ç‚º application/jsonã€‚"}) 
    
    user_id = (
        req.get("originalDetectIntentRequest", {})
        .get("payload", {})
        .get("data", {})
        .get("source", {})  # â† é€™è£¡å–å‡º dict
        .get("userId")      # â† å†å¾ dict ä¸­å–å‡º userId å­—ä¸²
        or
        req.get("originalDetectIntentRequest", {})
        .get("payload", {})
        .get("data", {})
        .get("events", [{}])[0]
        .get("source", {})
        .get("userId")
    )

    # print(f"ğŸ” è§£æå–å¾—çš„ user_id: {user_id}")
    
    # if user_id:
    #     push_to_line(user_id, "é€™æ˜¯å¾ GPT ä¸»å‹•æ¨æ’­çµ¦æ‚¨çš„è¨Šæ¯")
    # else:
    #     print("âŒ ç„¡æ³•å–å¾—ä½¿ç”¨è€… IDï¼Œæ¨æ’­å¤±æ•—")

    query_result = req.get("queryResult", {})
    user_query = query_result.get("queryText", "")
    session = req.get("session", "")
    intent = req.get("queryResult", {}).get("intent", {}).get("displayName", "")

    # è®€å– context ä¸­çš„åƒæ•¸
    context_params = {}
    for context in req.get("queryResult", {}).get("outputContexts", []):
        if "spec-context" in context.get("name", ""):
            context_params = context.get("parameters", {})

    def output_context(params):
        if not params or params.get("await_spec_selection") is False:
            # æ¸…é™¤ä¸Šä¸‹æ–‡
            return [{
                "name": f"{session}/contexts/spec-context",
                "lifespanCount": 0,  # è¨­ç½® lifespanCount ç‚º 0 æ¸…é™¤ä¸Šä¸‹æ–‡
                "parameters": {}
            }]
        else:
            # ä¿ç•™ä¸Šä¸‹æ–‡
            return [{
                "name": f"{session}/contexts/spec-context",
                "lifespanCount": 5,  # è¨­ç½®ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆæœŸ
                "parameters": params
            }]
   
    def generate_spec_reply(user_query, spec_data, spec_type_desc):
        keywords = {"è¦ç¯„", "è³‡æ–™", "æ¨™æº–åœ–", "æŸ¥è©¢", "æˆ‘è¦æŸ¥", "æŸ¥"}

        summary, matched_details, total_matches = search_piping_spec(user_query, spec_data, keywords)

        if total_matches == 0:
            english_query = translate_to_english(user_query)
            summary, matched_details, total_matches = search_piping_spec(english_query, spec_data, keywords)

        if total_matches > 0:
            reply = f"æ ¹æ“šã€Š{spec_type_desc}ã€‹ï¼Œæ‰¾åˆ° {total_matches} ç­†ç›¸é—œå…§å®¹ï¼š\n{summary}\nè«‹è¼¸å…¥å°æ‡‰çš„é …ç›®ç·¨è™ŸæŸ¥çœ‹è©³ç´°å…§å®¹ï¼ˆä¾‹å¦‚è¼¸å…¥ 1ï¼‰"
            
            context = {
                "await_spec_selection": True,
                "spec_options": list(matched_details.items())
            }

            return {
                "fulfillmentText": reply,
                "outputContexts": output_context({
                    "await_spec_selection": True,
                    "spec_options": list(matched_details.items())
                })
            }
        else:
            try:
                print("ğŸ” å‘¼å« GPT å›ç­”...")
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯é…ç®¡è¨­è¨ˆå°ˆå®¶ï¼Œåªå›ç­”èˆ‡é…ç®¡è¦ç¯„ç›¸é—œçš„å•é¡Œã€‚"},
                        {"role": "user", "content": user_query}
                    ],
                    max_tokens=350,
                    temperature=0.4,
                    top_p=1
                )
                reply = response.choices[0].message.content.strip()
            except Exception as e:
                print("âŒ GPT å‘¼å«å¤±æ•—:", e)
                reply = "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

            return {
                "fulfillmentText": reply
            }
    
    if context_params.get("await_spec_selection"):
        user_choice = user_query.strip()
        spec_items = context_params.get("spec_options", [])

        if not spec_items:
            return jsonify({
                "fulfillmentText": "ä¸Šä¸‹æ–‡å·²éæœŸï¼Œè«‹é‡æ–°æŸ¥è©¢ã€‚",
                "outputContexts": output_context({})
            })

        if user_choice.isdigit():
            index = int(user_choice) - 1
            if 0 <= index < len(spec_items):
                title, content = spec_items[index]

                # åˆ¤æ–·æ˜¯å¦è¶…é 300 å­—ï¼Œè‹¥è¶…éå‰‡å‘¼å« GPT é€²è¡Œé‡é»æ‘˜è¦
                if len(content) > 300:
                    try:
                        print("ğŸ“„ å…§å®¹è¶…é 300 å­—ï¼Œå‘¼å« GPT ç”Ÿæˆæ‘˜è¦ä¸­...")
                        response = client.chat.completions.create(
                            model="gpt-3.5-turbo",
                            messages=[
                                {"role": "system", "content": "ä½ æ˜¯é…ç®¡è¨­è¨ˆå°ˆå®¶ï¼Œè«‹å°‡ä»¥ä¸‹é…ç®¡è¦ç¯„å…§å®¹é€²è¡Œæ¢åˆ—å¼é‡é»æ•´ç†ï¼Œä¿ç•™åŸæ„ä¸¦æ¸…æ¥šç°¡æ˜ã€‚"},
                                {"role": "user", "content": content}
                            ],
                            max_tokens=400,
                            temperature=0.3,
                            top_p=0.8
                        )
                        summary = response.choices[0].message.content.strip()
                        reply_text = f"ğŸ“˜ æ‚¨é¸æ“‡çš„æ˜¯ï¼š{title}\n\nğŸ“Œ **é‡é»æ•´ç†ï¼š**\n{summary}\n\nğŸ“„ **åŸå§‹å…§å®¹å¦‚ä¸‹ï¼š**\n{content}"
                    except Exception as e:
                        print("âŒ GPT æ‘˜è¦å¤±æ•—:", e)
                        reply_text = f"ğŸ“˜ æ‚¨é¸æ“‡çš„æ˜¯ï¼š{title}\nå…§å®¹å¦‚ä¸‹ï¼š\n{content}"
                else:
                    reply_text = f"ğŸ“˜ æ‚¨é¸æ“‡çš„æ˜¯ï¼š{title}\nå…§å®¹å¦‚ä¸‹ï¼š\n{content}"

                return jsonify({
                    "fulfillmentText": reply_text,
                    "outputContexts": output_context({})  # æ¸…é™¤ä¸Šä¸‹æ–‡
                })
            else:
                return jsonify({
                    "fulfillmentText": f"è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—ï¼ˆä¾‹å¦‚ 1~{len(spec_items)}ï¼‰"
                })
        else:
            return jsonify({
                "fulfillmentText": "è«‹è¼¸å…¥é …ç›®ç·¨è™Ÿï¼ˆä¾‹å¦‚ 1 æˆ– 2ï¼‰ï¼Œä»¥æŸ¥çœ‹è©³ç´°å…§å®¹ã€‚"
            })

    if intent == "å•Ÿå‹•ç®¡ç·šç†±è™•ç†è¦ç¯„å•ç­”æ¨¡å¼":
        return jsonify({
            "fulfillmentText": ("è«‹å•æ‚¨æƒ³è©¢å•å“ªæ®µç†±è™•ç†è¦ç¯„å…§å®¹ï¼Ÿ\nä¾‹å¦‚ï¼šé ç†±æº«åº¦ã€PWHTæº«åº¦ã€ä¿æº«æ™‚é–“ã€å†·å»æ–¹å¼ç­‰ã€‚"),
            "outputContexts": output_context({
                "await_heat_question": True,                
            })
        })
    elif intent == "è«‹è¼¸å…¥ç®¡ç·šç­‰ç´šåç¨±":
        return jsonify({
                "fulfillmentText": ("è«‹è¼¸å…¥ç®¡ç·šç­‰ç´šï¼ˆå¦‚ A012ã€B012ã€A144N ç­‰ï¼‰ä»¥æŸ¥è©¢å°æ‡‰é€£çµã€‚"),
                "outputContexts": output_context({
                "await_pipinclass_download": True,                
            })
        })

    elif intent == "ä¸‹è¼‰ç®¡ç·šç­‰ç´š":
        extracted_data = extract_from_query(user_query)
        user_query = req.get("queryResult", {}).get("queryText", "").upper()
    # æ¯”å°ï¼š1 å€‹è‹±æ–‡å­—æ¯ + 3 ä½æ•¸å­— + å¯é¸çš„è‹±æ–‡å­—æ¯ï¼ˆå¦‚ A012ã€A144Nï¼‰
        match = re.search(r"\b([A-Z]{1,2}\d{2,4}[A-Z]?)\b", user_query.upper())
        if match:
            grade_code = match.group(1)
            if grade_code in type_links:
                return jsonify({
                    "fulfillmentText": f"é€™æ˜¯ç®¡ç·šç­‰ç´š {grade_code} çš„å°æ‡‰é€£çµï¼š\n{type_links[grade_code]}"
                })
            else:
                return jsonify({
                    "fulfillmentText": f"æ‰¾ä¸åˆ°ç®¡ç·šç­‰ç´š {grade_code} çš„é€£çµï¼Œè«‹ç¢ºèªæ˜¯å¦è¼¸å…¥æ­£ç¢ºã€‚"
                })
        else:
            return jsonify({
                "fulfillmentText": "è«‹è¼¸å…¥æ­£ç¢ºçš„ç®¡ç·šç­‰ç´šï¼ˆå¦‚ A012ã€B012ã€A144N ç­‰ï¼‰ä»¥æŸ¥è©¢å°æ‡‰é€£çµã€‚"
            })
        # return jsonify({
        #     "fulfillmentText": spec_reply.get_json()["fulfillmentText"],
        #     "outputContexts": output_context({
        #         "await_heat_question": True,
        #         "await_spec_selection": True
        #     })
        #})
    # elif intent == "è©¢å•é…ç®¡å…±åŒè¦æ±‚è¦ç¯„å…§å®¹":
    #     print(f"ğŸ” Debugè©¢å•é…ç®¡å…±åŒè¦æ±‚è¦ç¯„å…§å®¹: intent={intent}, user_query={user_query}, context_params={context_params}")
    #     spec_reply = generate_spec_reply(user_query, piping_specification, "è©¢å•é…ç®¡å…±åŒè¦æ±‚è¦ç¯„å…§å®¹")
    #     return jsonify(spec_reply)
    elif intent == "å•Ÿå‹•é…ç®¡å…±åŒè¦æ±‚è¦ç¯„å•ç­”æ¨¡å¼":
        return jsonify({
            "fulfillmentText": ("è«‹å•æ‚¨æƒ³è©¢å•å“ªæ®µé…ç®¡å…±åŒè¦æ±‚è¦ç¯„å…§å®¹"),
            "outputContexts": output_context({
                "await_pipecommon_question": True,                
            })
        })

    elif intent == "ç®¡æ”¯æ’è¦ç¯„":
        # çµ±ä¸€å–å¾—åƒæ•¸ï¼šå„ªå…ˆå¾ query æŠ½å‡ºï¼Œå¦å‰‡ä½¿ç”¨ context ä¸­å€¼
        extracted_data = extract_from_query(user_query)

        # æª¢æŸ¥æ˜¯å¦æåˆ° TYPE ç·¨è™Ÿ
        user_query = user_query.upper()  # é å…ˆè½‰å¤§å¯«ï¼Œæé«˜æ•ˆç‡

        if "TY" in user_query or re.search(r"M[-\s]*\d+", user_query):
            match_type = re.search(r"(?:TY(?:PE)?)[-\s]*0*(\d{1,3}[A-Z]?)", user_query.upper())
            match_m = re.search(r"(?:ç®¡æ”¯æ’\s*)?M[-\s]*0*(\d{1,2}[A-Z]?)", user_query.upper())

            if match_type:
                type_id = match_type.group(1)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰å­—æ¯å°¾ç¢¼ï¼Œä¸¦æ ¹æ“šæƒ…æ³è£œé›¶
                if type_id[-1].isalpha():
                    num_part = type_id[:-1].zfill(2) if type_id[:-1] else "00"
                    alpha_part = type_id[-1]
                    type_key = f"TYPE{num_part}{alpha_part}"
                else:
                    type_key = f"TYPE{type_id.zfill(2)}"

                if type_key in type_links:
                    return jsonify({
                        "fulfillmentText": f"é€™æ˜¯ç®¡æ”¯æ’è¦ç¯„ï¼ˆå¡‘åŒ–ï¼‰{type_key} çš„ä¸‹è¼‰é€£çµï¼š\n{type_links[type_key]}"
                    })
                else:
                    return jsonify({
                        "fulfillmentText": f"æ‰¾ä¸åˆ° {type_key} çš„å°æ‡‰é€£çµï¼Œè«‹ç¢ºèªæ˜¯å¦è¼¸å…¥æ­£ç¢ºã€‚"
                    })

            elif match_m:
                m_id = match_m.group(1)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰å­—æ¯å°¾ç¢¼ï¼Œä¸¦æ ¹æ“šæƒ…æ³è£œé›¶
                if m_id[-1].isalpha():
                    num_part = m_id[:-1].zfill(2) if m_id[:-1] else "00"
                    alpha_part = m_id[-1]
                    m_key = f"M{num_part}{alpha_part}"
                else:
                    m_key = f"M{m_id.zfill(2)}"

                if m_key in type_links:
                    return jsonify({
                        "fulfillmentText": f"é€™æ˜¯ç®¡æ”¯æ’è¦ç¯„ {m_key} çš„ä¸‹è¼‰é€£çµï¼š\n{type_links[m_key]}"
                    })
                else:
                    return jsonify({
                        "fulfillmentText": f"æ‰¾ä¸åˆ° {m_key} çš„å°æ‡‰é€£çµï¼Œè«‹ç¢ºèªæ˜¯å¦è¼¸å…¥æ­£ç¢ºã€‚"
                    })

            else:
                return jsonify({
                    "fulfillmentText": "è«‹è¼¸å…¥æ­£ç¢ºçš„ç®¡æ”¯æ’å‹å¼ç·¨è™Ÿï¼ˆå¦‚ TYPE01 æˆ– M01ï¼‰ä»¥æŸ¥è©¢è¦ç¯„é€£çµã€‚"
                })

    elif intent == "è©¢å•ç®¡ç·šç­‰ç´šå•é¡Œå›ç­”":
        try:
            print("ğŸ’¬ å•Ÿå‹• GPT è™•ç† pipeclass å•é¡Œï¼ˆå«PDFï¼‰...")
            reply = {
            "fulfillmentText": "ğŸ“„ æˆ‘æ­£åœ¨æŸ¥é–±ç›¸é—œæ–‡ä»¶ï¼Œè«‹ç¨å¾Œå¹¾ç§’...",
            "outputContexts": output_context({
                "await_heat_question": True
            })
        }
            # åŠ å…¥é¡å¤–åƒæ•¸: ä¾‹å¦‚æª”æ¡ˆID
            file_id = "file-Rx9uVCDFeBVp5sb7uC9VKU"
            Thread(target=process_gpt_logic, args=(user_query, user_id, intent, history, file_id)).start()
            
            return jsonify(reply)
        
        except Exception as e:
            print("âŒ GPT å‘¼å«å¤±æ•—:", e)
            reply = "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
            return jsonify({
                "fulfillmentText": reply
            })
   
    elif intent == "è¨­è¨ˆå•é¡Œé›†":

            # è®€å–æ­·å²ï¼ˆè‹¥è¶…é SESSION_TIMEOUT å‰‡é‡ç½®ï¼‰
            now = datetime.now()
            session_data = session_histories.get(session, {"messages": [], "last_seen": now})
            if now - session_data["last_seen"] > SESSION_TIMEOUT:
                session_data["messages"] = []

            # âœ… æª¢æŸ¥æ˜¯å¦è¦é‡è¨­å°è©±
            if user_query.strip() in ["é‡æ–°é–‹å§‹", "reset", "é‡è¨­å°è©±", "é‡æ–°ä¾†"]:
                session_data["messages"] = []
                session_data["last_seen"] = now
                session_histories[session] = session_data

                reply = {
                    "fulfillmentText": "âœ… å°è©±å·²é‡ç½®ï¼Œè«‹é‡æ–°è¼¸å…¥æ‚¨æƒ³æŸ¥è©¢çš„è¦ç¯„æˆ–å•é¡Œã€‚",
                }
                return jsonify(reply)

            history = session_data["messages"]

            # åŠ å…¥ä½¿ç”¨è€…è¼¸å…¥
            history.append({"role": "user", "content": user_query})

            # é™åˆ¶æ­·å²é•·åº¦
            if len(history) > MAX_HISTORY * 2:
                history = history[-MAX_HISTORY * 2:]

            # æ˜¯å¦éœ€è¦æé†’
            user_reminder = ""
            if len(history) >= MAX_HISTORY * 2:
                user_reminder = 'âš ï¸ æ‚¨çš„å°è©±å·²è¶…é 5 è¼ªï¼Œç‚ºä¿æŒæ•ˆèƒ½ï¼Œè«‹è¼¸å…¥"é‡è¨­å°è©±"ã€‚\n\n'

            session_data["messages"] = history
            session_data["last_seen"] = now
            session_histories[session] = session_data

            system_prompt = """
            ä½ æ˜¯é…ç®¡è¨­è¨ˆå°ˆå®¶ï¼Œå…·æœ‰åå¹´ä»¥ä¸Šå·¥æ¥­é…ç®¡ã€è¨­å‚™åŠé‹¼æ§‹è¨­è¨ˆç¶“é©—ï¼Œç†Ÿæ‚‰ASMEã€JISã€APIç­‰ç›¸é—œæ¨™æº–èˆ‡æ–½å·¥è¦ç¯„ã€‚
            å›ç­”æ™‚è«‹ä¿æŒå°ˆæ¥­ä¸”ç°¡æ½”æ˜ç­ï¼Œé¿å…éåº¦å†—é•·ã€‚
            å›ç­”å…§å®¹é ˆå…·é«”ä¸”æŠ€è¡“æ€§å¼·ï¼Œä¸¦ä»¥æ­£å¼ä¸”ç¦®è²Œçš„èªæ°£å›è¦†ã€‚
            å¦‚æœå•é¡Œè¶…å‡ºè¦ç¯„ç¯„åœï¼Œè«‹ç¦®è²Œå‘ŠçŸ¥ä¸¦å»ºè­°ç›¸é—œæŸ¥è©¢æ–¹å‘ã€‚
            è«‹é¿å…æä¾›èˆ‡å·¥ç¨‹è¨­è¨ˆç„¡é—œçš„è³‡è¨Šã€‚
            è«‹åœ¨å›ç­”ä¸­ç›¡é‡åŒ…å«æ¨™æº–ç·¨è™Ÿã€æ³•è¦æ¢æ–‡æˆ–æ¨™æº–åœ–å¼•ç”¨ã€‚
            è‹¥ä½¿ç”¨å°ˆæœ‰åè©ï¼Œè«‹é©ç•¶è§£é‡‹ä»¥ç¢ºä¿æ¸…æ™°æ˜“æ‡‚ã€‚
            """

            try:
                print("ğŸ’¬ ä½¿ç”¨ GPT èˆ‡å°è©±æ­·å²å›ç­”è¦ç¯„å•é¡Œ...")
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "system", "content": system_prompt}] + history,
                    max_tokens=400,
                    temperature=0.4,
                    top_p=1,                                
                    frequency_penalty=0.1,
                    presence_penalty=0,
                )
                reply = user_reminder + response.choices[0].message.content.strip()

                # å°‡ GPT å›ç­”åŠ å…¥æ­·å²
                history.append({"role": "assistant", "content": reply})
                session_data["messages"] = history
                session_data["last_seen"] = now
                session_histories[session] = session_data

            except Exception as e:
                print("âŒ GPT å‘¼å«å¤±æ•—:", e)
                reply = "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

            return jsonify({
                "fulfillmentText": reply
            })   

    elif intent == "Default Fallback Intent":

        # è®€å–æ­·å²ï¼ˆè‹¥è¶…é SESSION_TIMEOUT å‰‡é‡ç½®ï¼‰
        now = datetime.now()
        session_data = session_histories.get(session, {"messages": [], "last_seen": now})
        if now - session_data["last_seen"] > SESSION_TIMEOUT:
            session_data["messages"] = []

        # âœ… æª¢æŸ¥æ˜¯å¦è¦é‡è¨­å°è©±
        if user_query.strip() in ["é‡æ–°é–‹å§‹", "reset", "é‡è¨­å°è©±", "é‡æ–°ä¾†"]:
            session_data["messages"] = []
            session_data["last_seen"] = now
            session_histories[session] = session_data
            return jsonify({"fulfillmentText": "âœ… å°è©±å·²é‡ç½®ï¼Œè«‹é‡æ–°è¼¸å…¥æ‚¨æƒ³æŸ¥è©¢çš„è¦ç¯„æˆ–å•é¡Œã€‚"})

        history = session_data["messages"]

        # åŠ å…¥ä½¿ç”¨è€…è¼¸å…¥
        history.append({"role": "user", "content": user_query})

        # é™åˆ¶æ­·å²é•·åº¦
        if len(history) > MAX_HISTORY * 2:
            history = history[-MAX_HISTORY * 2:]

        # æ˜¯å¦éœ€è¦æé†’
        user_reminder = ""
        if len(history) >= MAX_HISTORY * 2:
            user_reminder = 'âš ï¸ æ‚¨çš„å°è©±å·²è¶…é 5 è¼ªï¼Œç‚ºä¿æŒæ•ˆèƒ½ï¼Œè«‹è¼¸å…¥"é‡è¨­å°è©±"ã€‚\n\n'

        session_data["messages"] = history
        session_data["last_seen"] = now
        session_histories[session] = session_data

        system_prompt = """
        ä½ æ˜¯é…ç®¡è¨­è¨ˆå°ˆå®¶ï¼Œå…·æœ‰åå¹´ä»¥ä¸Šå·¥æ¥­é…ç®¡ã€è¨­å‚™åŠé‹¼æ§‹è¨­è¨ˆç¶“é©—ï¼Œç†Ÿæ‚‰ASMEã€JISã€APIç­‰ç›¸é—œæ¨™æº–èˆ‡æ–½å·¥è¦ç¯„ã€‚
        å›ç­”æ™‚è«‹ä¿æŒå°ˆæ¥­ä¸”ç°¡æ½”æ˜ç­ï¼Œé¿å…éåº¦å†—é•·ã€‚
        å›ç­”å…§å®¹é ˆå…·é«”ä¸”æŠ€è¡“æ€§å¼·ï¼Œä¸¦ä»¥æ­£å¼ä¸”ç¦®è²Œçš„èªæ°£å›è¦†ã€‚
        å¦‚æœå•é¡Œè¶…å‡ºè¦ç¯„ç¯„åœï¼Œè«‹ç¦®è²Œå‘ŠçŸ¥ä¸¦å»ºè­°ç›¸é—œæŸ¥è©¢æ–¹å‘ã€‚
        è«‹é¿å…æä¾›èˆ‡å·¥ç¨‹è¨­è¨ˆç„¡é—œçš„è³‡è¨Šã€‚
        è«‹åœ¨å›ç­”ä¸­ç›¡é‡åŒ…å«æ¨™æº–ç·¨è™Ÿã€æ³•è¦æ¢æ–‡æˆ–æ¨™æº–åœ–å¼•ç”¨ã€‚
        è‹¥ä½¿ç”¨å°ˆæœ‰åè©ï¼Œè«‹é©ç•¶è§£é‡‹ä»¥ç¢ºä¿æ¸…æ™°æ˜“æ‡‚ã€‚
        """

        # è™•ç†ç‰¹å®šä¸Šä¸‹æ–‡é‚è¼¯ï¼ˆç†±è™•ç†ã€å…±åŒè¦ç¯„ã€ç®¡ç·šç­‰ç´šï¼‰
        if context_params.get("await_heat_question"):
            print("ğŸ”„ é‡æ–°è·¯ç”±åˆ°ç†±è™•ç†è¦ç¯„")
            spec_reply = generate_spec_reply(user_query, piping_heat_treatment, "è©¢å•ç†±è™•ç†è¦ç¯„")
            return jsonify(spec_reply)

        elif context_params.get("await_pipecommon_question"):
            print("ğŸ”„ é‡æ–°è·¯ç”±åˆ°é…ç®¡å…±åŒè¦ç¯„")
            spec_reply = generate_spec_reply(user_query, piping_specification, "è©¢å•é…ç®¡å…±åŒè¦ç¯„")
            return jsonify(spec_reply)

        elif context_params.get("await_pipinclass_download"):
            extracted_data = extract_from_query(user_query)
            user_query = req.get("queryResult", {}).get("queryText", "").upper()
        # æ¯”å°ï¼š1 å€‹è‹±æ–‡å­—æ¯ + 3 ä½æ•¸å­— + å¯é¸çš„è‹±æ–‡å­—æ¯ï¼ˆå¦‚ A012ã€A144Nï¼‰
            match = re.search(r"\b([A-Z]{1,2}\d{2,4}[A-Z]?)\b", user_query.upper())
            if match:
                grade_code = match.group(1)
                if grade_code in type_links:
                    return jsonify({
                        "fulfillmentText": f"é€™æ˜¯ç®¡ç·šç­‰ç´š {grade_code} çš„å°æ‡‰é€£çµï¼š\n{type_links[grade_code]}"
                    })
                else:
                    return jsonify({
                        "fulfillmentText": f"æ‰¾ä¸åˆ°ç®¡ç·šç­‰ç´š {grade_code} çš„é€£çµï¼Œè«‹ç¢ºèªæ˜¯å¦è¼¸å…¥æ­£ç¢ºã€‚"
                    })
            else:
                return jsonify({
                    "fulfillmentText": "è«‹è¼¸å…¥æ­£ç¢ºçš„ç®¡ç·šç­‰ç´šï¼ˆå¦‚ A012ã€B012ã€A144N ç­‰ï¼‰ä»¥æŸ¥è©¢å°æ‡‰é€£çµã€‚"
                })

        # ğŸ” è™•ç†å…¶ä»–è¦ç¯„å•é¡Œ
        elif context_params.get("await_pipeclass_question"):
            try:
                print("ğŸ’¬ ç”± GPT å›ç­”è¦ç¯„å…§å®¹...")
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_query}
                    ]+ history,
                    max_tokens=400,
                    temperature=0.3,
                    top_p=1,
                )
                reply = response.choices[0].message.content.strip()

            except Exception as e:
                print("âŒ GPT å‘¼å«å¤±æ•—:", e)
                reply = "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

            return jsonify({
                "fulfillmentText": reply
            })
        else :
            try:
                print("ğŸ’¬ ä½¿ç”¨ GPT èˆ‡å°è©±æ­·å²å›ç­”è¦ç¯„å•é¡Œ...")
                reply = {"fulfillmentText": f"ğŸ§  æˆ‘æ­£åœ¨æ€è€ƒä¸­ï¼Œè«‹ç¨å¾Œå¹¾ç§’..."}
                Thread(target=process_gpt_logic, args=(user_query, user_id, intent, history)).start()
                return jsonify(reply)

                # response = client.chat.completions.create(
                #     model="gpt-4o",
                #     messages=[
                #         {"role": "system", "content": system_prompt},
                #         {
                #             "role": "user",
                #             "content": [
                #                 {"type": "text", "text": user_query},
                #                 {"type": "file", "file": {"file_id": file_id}}
                #             ]
                #         }
                #     ],
                #     max_tokens=1000
                # )
            #     reply = user_reminder + response.choices[0].message.content.strip()

            #     # å°‡ GPT å›ç­”åŠ å…¥æ­·å²
            #     history.append({"role": "assistant", "content": reply})
            #     session_data["messages"] = history
            #     session_data["last_seen"] = now
            #     session_histories[session] = session_data

            except Exception as e:
                print("âŒ GPT å‘¼å«å¤±æ•—:", e)
                reply = "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

            return jsonify({
                "fulfillmentText": reply
            })   
 
    else: 
        return generate_spec_reply(user_query, piping_specification, "ä¼æ¥­é…ç®¡å…±åŒè¦ç¯„")


# def process_gpt_logic(user_query, user_id, intent, history):

#     try:
#         system_prompt = """
#         ä½ æ˜¯é…ç®¡è¨­è¨ˆå°ˆå®¶ï¼Œå…·æœ‰åå¹´ä»¥ä¸Šå·¥æ¥­é…ç®¡ã€è¨­å‚™åŠé‹¼æ§‹è¨­è¨ˆç¶“é©—ï¼Œç†Ÿæ‚‰ASMEã€JISã€APIç­‰ç›¸é—œæ¨™æº–èˆ‡æ–½å·¥è¦ç¯„ã€‚
#         å›ç­”æ™‚è«‹ä¿æŒå°ˆæ¥­ä¸”ç°¡æ½”æ˜ç­ï¼Œé¿å…éåº¦å†—é•·ã€‚
#         å›ç­”å…§å®¹é ˆå…·é«”ä¸”æŠ€è¡“æ€§å¼·ï¼Œä¸¦ä»¥æ­£å¼ä¸”ç¦®è²Œçš„èªæ°£å›è¦†ã€‚
#         å¦‚æœå•é¡Œè¶…å‡ºè¦ç¯„ç¯„åœï¼Œè«‹ç¦®è²Œå‘ŠçŸ¥ä¸¦å»ºè­°ç›¸é—œæŸ¥è©¢æ–¹å‘ã€‚
#         è«‹é¿å…æä¾›èˆ‡å·¥ç¨‹è¨­è¨ˆç„¡é—œçš„è³‡è¨Šã€‚
#         è«‹åœ¨å›ç­”ä¸­ç›¡é‡åŒ…å«æ¨™æº–ç·¨è™Ÿã€æ³•è¦æ¢æ–‡æˆ–æ¨™æº–åœ–å¼•ç”¨ã€‚
#         è‹¥ä½¿ç”¨å°ˆæœ‰åè©ï¼Œè«‹é©ç•¶è§£é‡‹ä»¥ç¢ºä¿æ¸…æ™°æ˜“æ‡‚ã€‚
#         """
#         messages = [{"role": "system", "content": system_prompt}] + history + [{"role": "user", "content": user_query}]

#         response = requests.post(
#             "https://api.openai.com/v1/chat/completions",
#             headers={
#                 "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
#                 "Content-Type": "application/json"
#             },
#             json={
#                 "model": "gpt-4o",
#                 "messages": messages,
#                 "max_tokens": 400,
#                 "temperature": 0.4,
#                 "top_p": 1
#             }
#         )
#         response_data = response.json()
#         reply = response_data["choices"][0]["message"]["content"].strip()

#         # ä½¿ç”¨ LINE Push API ä¸»å‹•æ¨é€çµæœ
#         push_to_line(user_id, reply)
#     except Exception as e:
#         print("âŒ GPT å‘¼å«å¤±æ•—:", e)
#         push_to_line(user_id, "æŠ±æ­‰ï¼Œç›®å‰ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")

def process_gpt_logic(user_query, user_id, intent, history, file_id=None):
    try:
        system_prompt = """
        ä½ æ˜¯é…ç®¡è¨­è¨ˆå°ˆå®¶ï¼Œç†Ÿæ‚‰å·¥æ¥­é…ç®¡ã€è¨­å‚™åŠé‹¼æ§‹è¨­è¨ˆï¼Œä¸¦èƒ½æ ¹æ“šé™„ä»¶çš„PDFæ–‡ä»¶å…§å®¹é€²è¡Œæº–ç¢ºå›ç­”ã€‚
        å›ç­”è¦å…·å°ˆæ¥­æ€§ã€ç°¡æ½”æ¸…æ¥šï¼Œå¿…è¦æ™‚å¼•ç”¨æ–‡ä»¶æ¢æ–‡èˆ‡æ¨™é¡Œï¼Œä¸¦èªªæ˜åƒè€ƒç¬¬å¹¾é ã€‚
        è‹¥è³‡æ–™ä¸åœ¨PDFä¸­ï¼Œè«‹æ˜ç¢ºå‘ŠçŸ¥ï¼›è‹¥æ²’æœ‰PDFï¼Œä¹Ÿè«‹æ ¹æ“šç¶“é©—æˆ–æ¨™æº–è¦ç¯„å›ç­”ã€‚
        """

        # å»ºç«‹ messages ä¸¦åŠ å…¥æ­·å²è¨Šæ¯
        messages = [{"role": "system", "content": system_prompt}]
        messages += history  # å°è©±æ­·å²åŠ é€²ä¾†

        # å»ºç«‹ä½¿ç”¨è€…é€™ä¸€è¼ªè¨Šæ¯
        user_message = [{"type": "text", "text": user_query}]
        if file_id:  # è‹¥æœ‰æª”æ¡ˆ ID æ‰åŠ å…¥
            user_message.append({"type": "file", "file": {"file_id": file_id}})
        
        messages.append({"role": "user", "content": user_message})

        # å‘¼å« GPT-4o API
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
                "Content-Type": "application/json"
            },
            json={
                "model": "gpt-4o",
                "messages": messages,
                "max_tokens": 800,
                "temperature": 0.4,
                "top_p": 1
            }
        )

        # å›è¦†è™•ç†
        response_data = response.json()
        reply = response_data["choices"][0]["message"]["content"].strip()
        push_to_line(user_id, reply)

    except Exception as e:
        print("âŒ GPT å‘¼å«å¤±æ•—:", e)
        push_to_line(user_id, "æŠ±æ­‰ï¼Œæˆ‘ç›®å‰ç„¡æ³•å®Œæˆæ­¤æŸ¥è©¢ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")

def push_to_line(user_id, reply):
    # ä½¿ç”¨ LINE Push API ä¸»å‹•æ¨é€çµæœ
    line_api_url = "https://api.line.me/v2/bot/message/push"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}"
    }
    payload = {
        "to": user_id,
        "messages": [{"type": "text", "text": reply}]
    }
    response = requests.post(line_api_url, headers=headers, json=payload)
    if response.status_code == 200:
        print("âœ… æˆåŠŸæ¨é€è¨Šæ¯è‡³ LINE")
    else:
        print(f"âŒ æ¨é€è¨Šæ¯å¤±æ•—ï¼š{response.status_code}, {response.text}")


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port)

# (Removed invalid Python code)ngrok http 5000